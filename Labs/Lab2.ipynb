{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing in Machine Learning\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Data Preprocessing Steps\n",
    "\n",
    "**Pipeline:** Raw Data → Cleaning → Splitting → Normalization → Feature Engineering → Imbalance Handling → Ready for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.6' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data from a CSV file\n",
    "data = pd.read_csv('path/to/your/data.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "\n",
    "---\n",
    "## 2. Removing Noise, Fixing Errors, and Ensuring Consistency\n",
    "\n",
    "### 2.1 Handling Missing Data (Imputation vs. Deletion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('path/to/your/data.csv')\n",
    "\n",
    "# Identify missing values\n",
    "missing_data = data.isnull().sum()\n",
    "print(missing_data)\n",
    "\n",
    "# Visualize missing data\n",
    "sns.heatmap(data.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title('Missing Data Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Imputation\n",
    "data_imputed = data.fillna(data.mean())\n",
    "\n",
    "# Deletion\n",
    "data_dropped = data.dropna()\n",
    "\n",
    "# Compare original, imputed, and dropped data\n",
    "print(\"Original Data Shape:\", data.shape)\n",
    "print(\"Imputed Data Shape:\", data_imputed.shape)\n",
    "print(\"Dropped Data Shape:\", data_dropped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2.2 Removing Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "data = data.drop_duplicates()\n",
    "print(\"Data Shape after Removing Duplicates:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2.3 Fixing Inconsistent Entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix inconsistent entries\n",
    "data['city'] = data['city'].replace({'NYC': 'New York City', 'SF': 'San Francisco'})\n",
    "print(data['city'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 3. Data Splitting\n",
    "\n",
    "### 3.1 Using train_test_split from sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training Set Shape:\", X_train.shape)\n",
    "print(\"Testing Set Shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Normalization and Standardization\n",
    "\n",
    "### 4.1 Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "# Visualize before and after normalization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data['numeric_column'], bins=30, kde=True)\n",
    "plt.title('Before Normalization')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(normalized_data[:, 0], bins=30, kde=True)\n",
    "plt.title('After Normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 4.2 Standardization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(data)\n",
    "\n",
    "# Visualize before and after standardization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data['numeric_column'], bins=30, kde=True)\n",
    "plt.title('Before Standardization')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(standardized_data[:, 0], bins=30, kde=True)\n",
    "plt.title('After Standardization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Feature Engineering\n",
    "\n",
    "### 5.1 Combining Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Creating BMI feature\n",
    "data['BMI'] = data['weight'] / (data['height'] ** 2)\n",
    "print(data[['weight', 'height', 'BMI']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 5.2 Encoding Categorical Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "data = pd.get_dummies(data, columns=['categorical_column'])\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 5.3 Log Transformation for Skewed Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transformation\n",
    "data['log_column'] = np.log(data['skewed_column'] + 1)\n",
    "\n",
    "# Visualize before and after log transformation\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data['skewed_column'], bins=30, kde=True)\n",
    "plt.title('Before Log Transformation')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(data['log_column'], bins=30, kde=True)\n",
    "plt.title('After Log Transformation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Feature Selection\n",
    "\n",
    "### 6.1 Statistical Methods: Correlation Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation_matrix = data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 6.2 Algorithmic Methods: Recursive Feature Elimination (RFE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# RFE\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, n_features_to_select=10)\n",
    "selected_features = rfe.fit_transform(X, y)\n",
    "print(\"Selected Features Shape:\", selected_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 6.3 Examples using Python Tools (SelectKBest, RFE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# SelectKBest\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "selected_features = selector.fit_transform(X, y)\n",
    "print(\"Selected Features Shape:\", selected_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Handling Imbalanced Data\n",
    "\n",
    "### 7.1 Sampling Techniques: Oversampling (SMOTE) and Undersampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# SMOTE\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Undersampling\n",
    "undersampler = RandomUnderSampler()\n",
    "X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
    "\n",
    "# Visualize class distribution before and after resampling\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x=y)\n",
    "plt.title('Before Resampling')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x=y_resampled)\n",
    "plt.title('After Resampling')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
